{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "active-romania",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset,DataLoader\n",
    "import pytorch_forecasting\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy import stats\n",
    "\n",
    "os.chdir(os.getcwd())\n",
    "fieldname = '_6hourly_20090101-20191231.npy'\n",
    "x1_arr = np.load('data_wind/z1000'+fieldname) # geopotential height data (9*6 resolution)\n",
    "x2_arr = np.load('data_wind/ua1000'+fieldname) # zonal wind data (9*6 resolution)\n",
    "x3_arr = np.load('data_wind/va1000'+fieldname) # meridional wind data (9*6 resolution)\n",
    "\n",
    "x1_arr = x1_arr[:,:6,:]\n",
    "x2_arr = x2_arr[:,:6,:]\n",
    "x3_arr = x3_arr[:,:6,:]\n",
    "\n",
    "x1_arr_norm = stats.zscore(x1_arr) # normalize\n",
    "x2_arr_norm = stats.zscore(x2_arr)\n",
    "x3_arr_norm = stats.zscore(x3_arr)\n",
    "y_arr = np.load('data_wind/stationwind_6hourly_20090101-20191231.npy') # rain data\n",
    "\n",
    "tensor_x = torch.Tensor(np.stack([x1_arr_norm,x2_arr_norm,x3_arr_norm],axis=1)) # join z,z and pv data\n",
    "#tensor_x = torch.Tensor(np.stack([x2_arr_norm,x3_arr_norm],axis=1)) # join z,z and pv data\n",
    "\n",
    "tensor_y = torch.Tensor(y_arr)\n",
    "\n",
    "forecast_dataset = TensorDataset(tensor_x,tensor_y) # creates a dataset based on tensors\n",
    "\n",
    "train_size = int(np.ceil(0.8*len(forecast_dataset)))\n",
    "train_set, val_set = torch.utils.data.random_split(forecast_dataset,[train_size,len(forecast_dataset)-train_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4ecdd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17cf3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataloader = DataLoader(train_set,batch_size=300,shuffle=True)\n",
    "valid_dataloader = DataLoader(val_set,batch_size=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8760bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_x.shape\n",
    "for x,y in training_dataloader:\n",
    "    print(x.shape,y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proper-credits",
   "metadata": {},
   "source": [
    "### activate autoreload so any changes you make to dataloader.py, model.py are automatically imported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "threaded-transcription",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3252aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_wind_nn import Net\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "material-banking",
   "metadata": {},
   "source": [
    "## the training:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "historic-promotion",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import tqdm\n",
    "\n",
    "loss_func = nn.MSELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sporting-score",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy_and_loss(dataloader,net):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    loss = 0\n",
    "    \n",
    "    mag_bins = torch.Tensor([0,1.5,3.3,5.5,7.9,10.7,13.8,17.1,20.7]) # Beaufort scale\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        net.cuda()\n",
    "    net.eval()\n",
    "    \n",
    "    n_batches = 0\n",
    "    with torch.no_grad():\n",
    "        for x,y in dataloader:\n",
    "            n_batches+=1\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                x = x.cuda()\n",
    "                y = y.cuda()\n",
    "            pred = net(x)\n",
    "            \n",
    "            loss+= loss_func(pred,y).item()\n",
    "            \n",
    "            obs_beau = torch.bucketize(y.norm(dim=1),mag_bins)\n",
    "            pred_beau = torch.bucketize(pred.norm(dim=1),mag_bins)\n",
    "            obs_deg = torch.atan(y[:,1]/y[:,0]) ; pred_deg = torch.atan(pred[:,1]/pred[:,0])\n",
    "            err_deg = 180*(obs_deg-pred_deg)/np.pi\n",
    "            err_deg = torch.abs((err_deg+180)%360-180)\n",
    "            \n",
    "            correct_mag = obs_beau == pred_beau\n",
    "            correct_deg = err_deg<=20\n",
    "            \n",
    "            # an accurate prediction is where the magnitude is the same on the Beaufort scale, \n",
    "            # and the direction is off by 20 degrees or less\n",
    "\n",
    "            correct_batch = np.logical_and(correct_mag,correct_deg)\n",
    "            \n",
    "            correct+=sum(correct_batch).item()\n",
    "            total+=len(y)\n",
    "            \n",
    "    loss = loss/n_batches      \n",
    "    return correct/total, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "standard-dallas",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_accuracy_and_loss(training_dataloader,net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "patent-little",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_accuracy_and_loss(valid_dataloader,net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6effbda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test loss\n",
    "pred = net(x)\n",
    "mag_bins = torch.Tensor([0,1.5,3.3,5.5,7.9,10.7,13.8,17.1,20.7]) # Beaufort scale for wind speed\n",
    "\n",
    "obs_beau = torch.bucketize(y.norm(dim=1),mag_bins)\n",
    "pred_beau = torch.bucketize(pred.norm(dim=1),mag_bins)\n",
    "obs_deg = torch.atan(y[:,1]/y[:,0]) ; pred_deg = torch.atan(pred[:,1]/pred[:,0])\n",
    "err_deg = 180*(obs_deg-pred_deg)/np.pi\n",
    "err_deg = torch.abs((err_deg+180)%360-180)\n",
    "\n",
    "correct_mag = obs_beau == pred_beau\n",
    "correct_deg = err_deg<=20\n",
    "\n",
    "# an accurate prediction is where the magnitude is the same on the Beaufort scale, \n",
    "# and the direction is off by 20 degrees or less\n",
    "correct_batch = np.logical_and(correct_mag,correct_deg)\n",
    "\n",
    "sum(correct_deg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55aa94cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(obs_beau, pred_beau)\n",
    "plt.imshow(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fcd654",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.linalg.norm(y_arr,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advisory-minority",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    net.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "featured-bradford",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 100\n",
    "\n",
    "training_loss_vs_epoch = []\n",
    "validation_loss_vs_epoch = []\n",
    "\n",
    "training_acc_vs_epoch = []\n",
    "validation_acc_vs_epoch = []\n",
    "\n",
    "pbar = tqdm.tqdm( range(n_epochs) )\n",
    "\n",
    "for epoch in pbar:\n",
    "    \n",
    "    loss,correct,total = 0,0,0\n",
    "    \n",
    "    if len(validation_loss_vs_epoch) > 1:\n",
    "        pbar.set_description('val acc:'+'{0:.5f}'.format(validation_acc_vs_epoch[-1])+\n",
    "                             ', train acc:'+'{0:.5f}'.format(training_acc_vs_epoch[-1]))\n",
    "    \n",
    "    net.train() # put the net into \"training mode\"\n",
    "    for x,y in training_dataloader:\n",
    "        \n",
    "        optimizer.zero_grad() # zero the gradient\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "\n",
    "        pred = net(x)\n",
    "        \n",
    "        cur_lossfunc = loss_func(pred,y)\n",
    "        loss+= cur_lossfunc.item()\n",
    "        \n",
    "        cur_lossfunc.backward()\n",
    "        optimizer.step()\n",
    "    net.eval() #put the net into evaluation mode\n",
    "    \n",
    "    \n",
    "    train_acc, train_loss = compute_accuracy_and_loss(training_dataloader,net)\n",
    "    valid_acc, valid_loss =  compute_accuracy_and_loss(valid_dataloader,net)\n",
    "         \n",
    "    training_loss_vs_epoch.append( train_loss)    \n",
    "    training_acc_vs_epoch.append( train_acc )\n",
    "    \n",
    "    validation_acc_vs_epoch.append(valid_acc)\n",
    "    \n",
    "    validation_loss_vs_epoch.append(valid_loss)\n",
    "    \n",
    "    #save the model if the validation loss has decreased\n",
    "#    if len(validation_loss_vs_epoch)==1 or validation_loss_vs_epoch[-2] > validation_loss_vs_epoch[-1]:\n",
    "#        torch.save(net.state_dict(), 'trained_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inclusive-wrestling",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,2,figsize=(8,3))\n",
    "\n",
    "ax[0].plot(training_loss_vs_epoch,label='training')\n",
    "ax[0].plot(validation_loss_vs_epoch,label='validation')\n",
    "\n",
    "ax[1].plot(training_acc_vs_epoch)\n",
    "ax[1].plot(validation_acc_vs_epoch)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63268b4",
   "metadata": {},
   "source": [
    "Nearest Neighbor Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a5b377",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "neigh = KNeighborsRegressor(n_neighbors=3)\n",
    "neigh.fit(torch.flatten(train_set.dataset.tensors[0],1,-1),train_set.dataset.tensors[1])\n",
    "pred_neigh = torch.Tensor(neigh.predict(torch.flatten(val_set.dataset.tensors[0],1,-1)))\n",
    "y_neigh = val_set.dataset.tensors[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21e4a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_neigh = 0\n",
    "total_neigh = 0\n",
    "obs_beau_neigh = torch.bucketize(y_neigh.norm(dim=1),mag_bins)\n",
    "pred_beau_neigh = torch.bucketize(pred_neigh.norm(dim=1),mag_bins)\n",
    "obs_deg_neigh = torch.atan(y_neigh[:,1]/y_neigh[:,0]) ; pred_deg_neigh = torch.atan(pred_neigh[:,1]/pred_neigh[:,0])\n",
    "err_deg_neigh = 180*(obs_deg_neigh-pred_deg_neigh)/np.pi\n",
    "err_deg_neigh = torch.abs((err_deg_neigh+180)%360-180)\n",
    "\n",
    "correct_mag_neigh = obs_beau_neigh == pred_beau_neigh\n",
    "correct_deg_neigh = err_deg_neigh<=20\n",
    "\n",
    "correct_batch_neigh = torch.logical_and(correct_mag_neigh,correct_deg_neigh)\n",
    "\n",
    "total_neigh+=len(y_neigh)\n",
    "sum(correct_batch_neigh)/total_neigh"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
